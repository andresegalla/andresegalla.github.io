

<!DOCTYPE html>


<html lang="en" data-content_root="" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>Loss Functions from a Bayesian perspective &#8212; Loss Functions from a Bayesian perspective</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=5b4479735964841361fd" rel="stylesheet" />
<link href="_static/styles/bootstrap.css?digest=5b4479735964841361fd" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=5b4479735964841361fd" rel="stylesheet" />

  
  <link href="_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=5b4479735964841361fd" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" href="_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=5b4479735964841361fd" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=5b4479735964841361fd" />
  <script src="_static/vendor/fontawesome/6.1.2/js/all.min.js?digest=5b4479735964841361fd"></script>

    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script src="_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'loss_functions';</script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="prev" title="Introduction" href="intro.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <div id="pst-scroll-pixel-helper"></div>

  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>
    Back to top
  </button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  

<a class="navbar-brand logo" href="intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="_static/logo.png" class="logo__image only-light" alt="Loss Functions from a Bayesian perspective - Home"/>
    <script>document.write(`<img src="_static/logo.png" class="logo__image only-dark" alt="Loss Functions from a Bayesian perspective - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="intro.html">
                    Introduction
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Loss Functions from a Bayesian perspective</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/executablebooks/jupyter-book" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/executablebooks/jupyter-book/issues/new?title=Issue%20on%20page%20%2Floss_functions.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="_sources/loss_functions.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Loss Functions from a Bayesian perspective</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#predictive-distribution">Predictive Distribution</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#maximum-a-posteriori-estimate-map">Maximum a Posteriori estimate (MAP)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#entropy">Entropy</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="loss-functions-from-a-bayesian-perspective">
<h1>Loss Functions from a Bayesian perspective<a class="headerlink" href="#loss-functions-from-a-bayesian-perspective" title="Permalink to this heading">#</a></h1>
<p>Let’s supposed we have <span class="math notranslate nohighlight">\(N\)</span> data points available <span class="math notranslate nohighlight">\(D=\{(x_i, y_i)\}_{i=1} ^ N\)</span>, where <span class="math notranslate nohighlight">\(x_i \in \mathcal{X}\)</span> are the <em>inputs</em> and <span class="math notranslate nohighlight">\(y_i \in \mathcal{Y}\)</span> the respetive <em>outputs</em>, or <em>targets</em>, and we want to learn some relationship, denoted by <span class="math notranslate nohighlight">\(f\)</span>, between inputs and targets:</p>
<div class="math notranslate nohighlight">
\[f : \mathcal{X} \to \mathcal{Y}\]</div>
<p>This is a general <strong>supervised learning setting</strong>. Since we almost never have <em>complete information</em> and our data is corrupted by all kinds of <em>noises</em>, it is natural to include uncertainty into the modelling process by using probabilities. <em>Estimates without any uncertainty statement are meaningless</em>.</p>
<p>With that let’s just define some notation:</p>
<ul class="simple">
<li><p>Capital letters <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> are <strong><a class="reference external" href="https://en.wikipedia.org/wiki/Random_variable">random variables</a></strong>, while small letters <span class="math notranslate nohighlight">\(x\)</span>, <span class="math notranslate nohighlight">\(y\)</span> are <em>realizations</em> of the random variables <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span>, respectively.</p></li>
</ul>
<p>Finally, we define:</p>
<div class="math notranslate nohighlight">
\[p(y|x) := Prob(Y=y | X=x),\]</div>
<p>as the <em>conditional probability</em> of observing a <span class="math notranslate nohighlight">\(Y=y\)</span> <em>given</em> that <span class="math notranslate nohighlight">\(X=x\)</span> is observed. <strong>So, our supervised learning problem is precisely to estimate <span class="math notranslate nohighlight">\(p(y|x)\)</span>, given all available information at hand</strong>, which we denote by <span class="math notranslate nohighlight">\(I\)</span>.
Formally we should therefore write as <span class="math notranslate nohighlight">\(p(y|x, I)\)</span>, but we’ll omit it to simplify notation. In case of continuous <span class="math notranslate nohighlight">\(y\)</span>, <span class="math notranslate nohighlight">\(p(y|x)\)</span> is usually called <em>probability densisity function</em> (pdf), while for the case of discrete <span class="math notranslate nohighlight">\(y\)</span>, <span class="math notranslate nohighlight">\(p(y|x)\)</span> is a <em>probability mass function</em> (pmf).</p>
<p>Now, what is <span class="math notranslate nohighlight">\(I\)</span> and how do we leverage it? As said above, <span class="math notranslate nohighlight">\(I\)</span> is whatever information about the problem we have at hand and can use to constrain our problem. The most obvious piece of relevant information is of course the available data <span class="math notranslate nohighlight">\(D\)</span>. Also, in order to be able to proceed, we assume some kind of <strong>parametrization</strong> of <span class="math notranslate nohighlight">\(p(y|x)\)</span> to constrain the solution space and to allow us using efficient optimization algorithms to find good solutions. The choice of the parametrization is crucial and is what leads to the form of the main term of the Loss function. Let’s write this as follows:</p>
<div class="math notranslate nohighlight" id="equation-parametrization">
<span class="eqno">(1)<a class="headerlink" href="#equation-parametrization" title="Permalink to this equation">#</a></span>\[p(y|x) = p(y | \lambda (x)),\]</div>
<p>where <span class="math notranslate nohighlight">\(p\)</span> is now parametrized by <span class="math notranslate nohighlight">\(\lambda\)</span>, which can depend on <span class="math notranslate nohighlight">\(x\)</span>. <a class="reference internal" href="#equation-parametrization">(1)</a> is called the <strong>likelihood function</strong>.</p>
<p>As example, supposing both <span class="math notranslate nohighlight">\(x\)</span> and <span class="math notranslate nohighlight">\(y\)</span> are real numbers, one could use the <em>gaussian distribution</em>, where <span class="math notranslate nohighlight">\(\lambda (x) = (\mu (x), \sigma (x))\)</span> are the expectation and standard deviation, respectively :</p>
<div class="math notranslate nohighlight" id="equation-gaussian">
<span class="eqno">(2)<a class="headerlink" href="#equation-gaussian" title="Permalink to this equation">#</a></span>\[p(y|x) = \mathcal{N}(y | \mu(x), \sigma(x)),\]</div>
<p>Our goal in this case would be to estimate <span class="math notranslate nohighlight">\(\mu(x)\)</span> and  <span class="math notranslate nohighlight">\(\sigma(x)\)</span>. Maybe in most cases people tend to care only about <span class="math notranslate nohighlight">\(\mu(x)\)</span> and completely ignore <span class="math notranslate nohighlight">\(\sigma(x)\)</span>.</p>
<p>Now, in ML what we do is to build some <strong>model</strong> to predict the parameters <span class="math notranslate nohighlight">\(\lambda (x)\)</span>. The model is in turn a function, <span class="math notranslate nohighlight">\(m\)</span>, that is parametrized by its own parameters <span class="math notranslate nohighlight">\(\theta\)</span>:</p>
<div class="math notranslate nohighlight">
\[\lambda (x) = m(x | \theta).\]</div>
<p><span class="math notranslate nohighlight">\(m\)</span> can be any suitable ML model, like neural networks, decision trees, boosting, etc.
Fitting or training a model means finding a suitable set of parameters <span class="math notranslate nohighlight">\(\theta\)</span>. When we find <span class="math notranslate nohighlight">\(\theta\)</span>, we have in principle solved our problem, because we easily get to what we initially wanted to estimate, which is the conditional distribution <span class="math notranslate nohighlight">\(p(y|x)\)</span>:</p>
<div class="math notranslate nohighlight">
\[\theta \to \lambda \to p(y|x).\]</div>
<p>With that, we can finally re-write the conditional distribution as:</p>
<div class="math notranslate nohighlight">
\[p(y|x) = p(y | \lambda (x)) = p(y | x, \theta),\]</div>
<p>where we replaced the <span class="math notranslate nohighlight">\(\lambda (x)\)</span> in the conditioning by <span class="math notranslate nohighlight">\(x, \theta\)</span> and omitted the dependence on the model function <span class="math notranslate nohighlight">\(m\)</span> to simplify notation.</p>
<p>In short, the information <span class="math notranslate nohighlight">\(I\)</span> we are leveraging sor far to frame our problem in some solvable way could be written as:</p>
<div class="math notranslate nohighlight">
\[I = \{\text{available data}, \text{data parametrization}, \text{model parametrization} \} = \{D, \lambda, \theta \}.\]</div>
<section id="predictive-distribution">
<h2>Predictive Distribution<a class="headerlink" href="#predictive-distribution" title="Permalink to this heading">#</a></h2>
<p>From a full Bayesian perspective we can write:</p>
<div class="math notranslate nohighlight" id="equation-predictive-dist">
<span class="eqno">(3)<a class="headerlink" href="#equation-predictive-dist" title="Permalink to this equation">#</a></span>\[p(y | x, D) = \int p(y, \theta | x, D) d\theta = \int p(y | x, \theta)p(\theta | D) d\theta, \]</div>
<p>where in the first equality we used marginalization over the model parameters <span class="math notranslate nohighlight">\(\theta\)</span> and in the second equality we used the <a class="reference external" href="https://en.wikipedia.org/wiki/Chain_rule_(probability)">product rule</a>. Moreover, the term <span class="math notranslate nohighlight">\(p(\theta | D)\)</span> is called the <strong>posterior</strong> probablity distribution of <span class="math notranslate nohighlight">\(\theta\)</span> given <span class="math notranslate nohighlight">\(D\)</span>, which can be understood as the probability of some model (parametrized by <span class="math notranslate nohighlight">\(\theta\)</span>) in light of the available data <span class="math notranslate nohighlight">\(D\)</span>. What we are doing in <a class="reference internal" href="#equation-predictive-dist">(3)</a> is obtaining our final estimate <span class="math notranslate nohighlight">\(p(y | x, D)\)</span> as an average of the likelihood function weighted by the likelihood of potential models describing the data relationship. Formally this completely fine, but it turns out that in practice it is usually not feasible (or to costly) to compute the integral over model parameters in <a class="reference internal" href="#equation-predictive-dist">(3)</a>.</p>
</section>
<section id="maximum-a-posteriori-estimate-map">
<h2>Maximum a Posteriori estimate (MAP)<a class="headerlink" href="#maximum-a-posteriori-estimate-map" title="Permalink to this heading">#</a></h2>
<p>In order to bypass the complexity of computing <a class="reference internal" href="#equation-predictive-dist">(3)</a>, we could choose some specific “best” model in some sense, given by some <span class="math notranslate nohighlight">\(\theta^*\)</span>. But which one? Maybe the simplest one would be: <em>the most likely, given our data</em>. We can write precisely that as:</p>
<div class="math notranslate nohighlight">
\[\theta^* =  \underset{\theta}{\text{argmax }}  p(\theta | D).\]</div>
<p><span class="math notranslate nohighlight">\(p(\theta | D)\)</span> is the <strong>posterior</strong> distribution of <span class="math notranslate nohighlight">\(\theta\)</span> given <span class="math notranslate nohighlight">\(D\)</span> and <span class="math notranslate nohighlight">\(\theta^*\)</span> is called the <strong>Maximum a Posteriori (MAP)</strong> estimate. We can rewrite <span class="math notranslate nohighlight">\(p(\theta | D)\)</span> using <a class="reference external" href="https://en.wikipedia.org/wiki/Bayes%27_theorem">Bayes’ theorem</a>:</p>
<div class="math notranslate nohighlight">
\[p(\theta | D) = \frac{p(D|\theta)p(\theta)}{p(D)},\]</div>
<p>where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(p(D|\theta)\)</span> is the <strong>likelihood function</strong>,</p></li>
<li><p><span class="math notranslate nohighlight">\(p(\theta)\)</span> is the <strong>prior</strong>,</p></li>
<li><p><span class="math notranslate nohighlight">\(p(D) = \int  p(D|\theta)p(\theta) d\theta\)</span> is the <strong>evidence</strong>.</p></li>
</ul>
<p>To increase numerical stability, it is convenient to maximize the <strong>log</strong> of the posterior (which does not change the maximum):</p>
<div class="math notranslate nohighlight">
\[\log p(\theta | D) = \log p(D|\theta) + \log p(\theta) - \log p(D).\]</div>
<p>Since maximizing a function is equivalent to minimizing its opposite, we can identify the negative log posterior as the Loss Function <span class="math notranslate nohighlight">\(\mathcal{L}\)</span> of our problem:</p>
<div class="math notranslate nohighlight">
\[Loss = \mathcal{L}(\theta, D) := - \log  p(\theta | D).\]</div>
<p>Further, if we can assume that all <span class="math notranslate nohighlight">\(\{(x_i, y_i\)</span>)} are (i.i.d), we have:</p>
<div class="math notranslate nohighlight">
\[\log p(D|\theta) = \log \prod\limits_{i} p(y_i, x_i|\theta) = \log \prod\limits_{i} p(y_i|x_i,\theta)p(x_i) = \sum\limits_{i} \log p(y_i|x_i,\theta) + \sum\limits_{i} p(x_i),\]</div>
<p>which leads us to:</p>
<div class="math notranslate nohighlight">
\[ \mathcal{L}(\theta, D) = - \sum\limits_{i} \log p(y_i|x_i,\theta) - \log p(\theta) + K,\]</div>
<p>where <span class="math notranslate nohighlight">\(K\)</span> is a constant that does not depend on <span class="math notranslate nohighlight">\(\theta\)</span> and is therefore irrelevant for our optimization and can be ignored.</p>
</section>
<section id="entropy">
<h2>Entropy<a class="headerlink" href="#entropy" title="Permalink to this heading">#</a></h2>
<p>Just as an interesting side note, let us just quickly rewrite the log likelihood term as follows:</p>
<div class="math notranslate nohighlight">
\[\log p(D|\theta) = \log \prod\limits_{i}^N p(y_i, x_i|\theta) = \sum \limits_{i}^N \log p(y_i, x_i|\theta).\]</div>
<p>It turns out that if we divide the last sum by <span class="math notranslate nohighlight">\(N\)</span>, it is precisely an estimate of the expectation of <span class="math notranslate nohighlight">\(\log p\)</span>:</p>
<div class="math notranslate nohighlight">
\[\langle \log p(x, y | \theta) \rangle \approx \frac{1}{N}\sum \limits_{i}^N \log p(y_i, x_i|\theta).\]</div>
<p>But minus the expected value of the <span class="math notranslate nohighlight">\(\log p(x,y)\)</span> is precisely the definition of the <a class="reference external" href="https://en.wikipedia.org/wiki/Joint_entropy">joint differential entropy</a>, <span class="math notranslate nohighlight">\(h(X,Y)\)</span>:</p>
<div class="math notranslate nohighlight">
\[h(X, Y | \theta) := -\langle \log p(x, y | \theta) \rangle = -\int\limits_{\mathcal{X}, \mathcal{Y}} p(x,y | \theta) \log p(x,y | \theta) dxdy.\]</div>
<p>So, this is basically saying that <strong>maximizing the log likelihood function is equivalent to minimizing the joint differential entropy</strong>. This makes sense if we think about entropy as measuring our lack of knowledge (ignorance) about random variables: <em>when we fit a model to describe our data, we want it to capture as much information about it as possible, or, equivalently, we want it to reduce our ignorance about it as much as possible</em>.</p>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="intro.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Introduction</p>
      </div>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#predictive-distribution">Predictive Distribution</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#maximum-a-posteriori-estimate-map">Maximum a Posteriori estimate (MAP)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#entropy">Entropy</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Andre Segalla
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2022.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/bootstrap.js?digest=5b4479735964841361fd"></script>
<script src="_static/scripts/pydata-sphinx-theme.js?digest=5b4479735964841361fd"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>